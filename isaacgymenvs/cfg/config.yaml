
# Task name - used to pick the class to load
task_name: ${task.name}
# experiment name. defaults to name of training config
experiment: ''

# if set to positive integer, overrides the default number of environments
num_envs: ''

# seed - set to -1 to choose random seed
seed: 42
# set to True for deterministic performance
torch_deterministic: False

# set the maximum number of learning iterations to train for. overrides default per-environment setting
max_iterations: ''

## Device config
#  'physx' or 'flex'
physics_engine: 'physx'
# whether to use cpu or gpu pipeline
pipeline: 'gpu'
# device for running physics simulation
sim_device: 'cuda:0'
# device to run RL
rl_device: 'cuda:0'
graphics_device_id: 0

## PhysX arguments
num_threads: 4 # Number of worker threads per scene used by PhysX - for CPU PhysX only.
solver_type: 1 # 0: pgs, 1: tgs
num_subscenes: 4 # Splits the simulation into N physics scenes and runs each one in a separate thread

# RLGames Arguments
# test - if set, run policy in inference mode (requires setting checkpoint to load)
test: False
# used to set checkpoint path
checkpoint: ''
# set to True to use multi-gpu horovod training
multi_gpu: False

wandb_activate: False
wandb_group: ''
wandb_name: ${train.params.config.name}
wandb_entity: 'jinseokbae'
wandb_project: 'pmp'
capture_video: False
capture_video_freq: 1464
capture_video_len: 100
force_render: True

# disables rendering
headless: False

# set default task and default training config based on task
defaults:
  - task: Ant
  - train: ${task}PPO
  - hydra/job_logging: disabled

# set the directory where the output files get saved
hydra:
  output_subdir: null
  run:
    dir: .

# set scenario config
scenario: "loco"

# kinematics mode
kinematics_mode: False

# whether use dof_obs_separate_amp or not
separate_dof_mode: 1

# whether print discriminator prediction or not
print_disc: False

# taskw : discw
taskw: 0.5
discw: 0.5

# whether using aip or not
using_aip: True
separate_aip_mode: 0

# for cart mode
cart_mode: "plain_rand"

# for walking imitation
walking_style: "normal"

# for squat
squat_mode: "sumo"
squat_rew_style: "subgoal"

# for jumpcarry
carry_mode: "normal"
stable_carry_style: "none"

# discriminator related experiment
disc_grad_penalty: 5
amp_demo_blend_prob: 0.0
aip_demo_blend_prob: 0.0
amp_reward_offset: 0.0
aip_reward_offset: 0.0

# (dglim) for visualization
viewing_targets: ["whole_body"]
target_episodes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
output_dir: ""

# training
gamma: 0.99