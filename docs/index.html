<!DOCTYPE html>
<html data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>PMP SIGGRAPH 2023</title>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <!--
    <link
      href="https://fonts.googleapis.com/css2?family=Encode+Sans:wght@300;400;500;600&family=Roboto+Mono&display=swap"
      rel="stylesheet"
    />
    <link href='https://fonts.googleapis.com/css?family=Architects Daughter' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Quicksand' rel='stylesheet'>
    -->
    <link href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200..1000;1,200..1000&family=Roboto+Mono&display=swap" rel="stylesheet">

    <!-- Bulma -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.1/css/bulma.min.css" />
    <link rel="stylesheet" href="./css/styles.css" />
    <script src="./js/bulma_toggle.js"></script>

    <!-- Font Awesome -->
    <script src="https://kit.fontawesome.com/5fd1dd8417.js" crossorigin="anonymous"></script>

    <!-- Academicons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-touch-icon.png"/>
    <link rel="icon" sizes="32x32" href="./assets/favicon-32x32.png"/>
    <link rel="icon" sizes="16x16" href="./assets/favicon-16x16.png"/>

    <!-- MathJax -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>

  <body>

    <!-- title / authors / icons -->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-widescreen has-text-centered">
          <!-- title -->
          <h1 class="title is-size-1 is-size-2-mobile publication-title">
            PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors
          </h1>
          <div class="is-size-4-tablet publication-institute">
            <span class="author-block">SIGGRAPH 2023<br/></span>
          </div>

          <!-- authors -->
          <div class="container is-max-desktop has-text-centered author-list">
            <div class="columns is-mobile is-centered is-gapless">
              <div class="column is-2-tablet is-size-5-tablet publication-authors">
                <a class="author-blocks" href="https://jinseokbae.github.io">Jinseok Bae</a>
              </div>
              <div class="column is-2-tablet is-size-5-tablet publication-authors">
                <a class="author-blocks" href="https://sites.google.com/view/jungdam/home">Jungdam Won</a>
              </div>
              <div class="column is-2-tablet is-size-5-tablet publication-authors">
                <a class="author-blocks" href="https://3d.snu.ac.kr/members">Donggeun Lim</a>
              </div>
              <div class="column is-2-tablet is-size-5-tablet publication-authors">
                <a class="author-blocks" href="https://3d.snu.ac.kr/members">Cheol-Hui Min</a>
              </div>
              <div class="column is-2-tablet is-size-5-tablet publication-authors">
                <a class="author-blocks" href="http://3d.snu.ac.kr/members">Young Min Kim</a>
              </div>
            </div>
          </div>

          <div class="is-size-5-tablet publication-institute">
            <span class="author-block">Seoul National University</span>
          </div>
          
          <div class="logo-list">
            <a class="lab-logo" href="https://3d.snu.ac.kr">
              <img src="./assets/3dv.png">
            </a>
          </div>

          <!-- icons -->
          <div class="is-size-5 link-blocks">
            <a class="button link-button is-rounded" href="https://dl.acm.org/doi/10.1145/3588432.3591487">
              <span class="icon">
                <i class="fa-solid fa-file"></i>
              </span>
              <span>Paper</span>
            </a>
            <a class="button link-button is-rounded" href="https://arxiv.org/pdf/2305.03249">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
            <a class="button link-button is-rounded" href="https://www.youtube.com/watch?v=WdLGvKdNG-0">
              <span class="icon">
                <i class="fa-brands fa-youtube"></i>
              </span>
              <span>Video</span>
            </a>
            <a class="button link-button is-rounded" href="https://github.com/jinseokbae/pmp">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </section>

    <!-- teasor -->
    <section class="hero">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="./assets/teaser.png" width="80%" style="display: block; margin: auto" />
          <h2 class="tldr">
            <i>Our method utilizes multiple <b>part-wise motion priors</b> to interact with the environments.</i>
          </h2>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="column is-full-width is-centered has-text-centered">
          <h2 class="subtitle is-size-3 has-text-weight-medium publication-keywords">
            Abstract
          </h2>
          <div class="content has-text-justified">
            <p>
              We present a method to animate a character incorporating multiple part-wise motion priors (PMP). 
              While previous works allow creating realistic articulated motions from reference data, the range of motion is largely limited by the available samples.
              Especially for the interaction-rich scenarios, it is impractical to attempt acquiring every possible interacting motion, as the combination of physical parameters increases exponentially.
              The proposed PMP allows us to assemble multiple part skills to animate a character, creating a diverse set of motions with different combinations of existing data.
              In our pipeline, we can train an agent with a wide range of part-wise priors.
              Therefore, each body part can obtain a kinematic insight of the style from the motion captures, or at the same time extract dynamics-related information from the additional part-specific simulation.
              For example, we can first train a general interaction skill, e.g. grasping, only for the dexterous part, and then combine the expert trajectories from the pre-trained agent with the kinematic priors of other limbs.
              Eventually, our whole-body agent learns a novel physical interaction skill even with the absence of the object trajectories in the reference motion sequence.
            </p>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="column is-full-width is-centered has-text-centered">
          <h2 class="subtitle is-size-3 has-text-weight-medium publication-keywords">
            Method
          </h2>
          <img src="./assets/pipeline.jpg" width="50%" style="display: block; margin: auto" />
        </div>
          <p class="normal-text has-text-justified">
            Visualization of the pipeline in our system. 
            <b><font color='grey'>Kinematic style discriminators</font></b> \(\{D_{\phi_k}\}_K\) are trained with part-wise motion captures and <b><font color='grey'>interaction discriminators</font></b> \(\{D_{\psi_n}\}_N\) are trained with demo trajectories from the pretrained interaction gym. 
            Note partial observations \(\{o_k\}_K,\{u_n\}_N\) and hand actions \(\{y_n\}_N\) are subsets of state \(s\) and action \(a\) of the whole-body agent.
          </p>
      </div>
      </section>

    
    <section class="section">
      <div class="container is-max-desktop">
        <div class="column is full-width is-centered has-text-centered">
          <h2 class="subtitle is-size-3 has-text-weight-medium publication-keywords">
            Video
          </h2>
        </div>
        <div class="publication-video">
          <iframe
            src="./assets/intro_video.mp4"
            allow="autoplay; encrypted-media"
            allowfullscreen="true"
          ></iframe>
        </div>
      </div>
    </section>

    <!-- BibTex sectoion -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="column is-full-width is-centered has-text-centered">
          <h2 class="subtitle is-size-3 has-text-weight-medium publication-keywords">BibTeX</h2>
        </div>
        <div class="box bibtex-box">
          <pre>
@inproceedings{10.1145/3588432.3591487,
author = {Bae, Jinseok and Won, Jungdam and Lim, Donggeun and Min, Cheol-Hui and Kim, Young Min},
title = {PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591487},
doi = {10.1145/3588432.3591487},
abstract = {We present a method to animate a character incorporating multiple part-wise motion priors (PMP). While previous works allow creating realistic articulated motions from reference data, the range of motion is largely limited by the available samples. Especially for the interaction-rich scenarios, it is impractical to attempt acquiring every possible interacting motion, as the combination of physical parameters increases exponentially. The proposed PMP allows us to assemble multiple part skills to animate a character, creating a diverse set of motions with different combinations of existing data. In our pipeline, we can train an agent with a wide range of part-wise priors. Therefore, each body part can obtain a kinematic insight of the style from the motion captures, or at the same time extract dynamics-related information from the additional part-specific simulation. For example, we can first train a general interaction skill, e.g. grasping, only for the dexterous part, and then combine the expert trajectories from the pre-trained agent with the kinematic priors of other limbs. Eventually, our whole-body agent learns a novel physical interaction skill even with the absence of the object trajectories in the reference motion sequence.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {64},
numpages = {10},
keywords = {Data-driven Animation, Deep Reinforcement Learning, Physics-Based Simulation, Whole-body Control},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}
          </pre>
        </div>
      </div>
    </section>

    <!-- Footer section -->
    <footer class="footer" style="padding-top: 1rem">
      <!-- navigation -->
      <a role="button" class="navbar-burger" data-target="moreResearch" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
      <div class="navbar-menu" id="moreResearch">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center">
          <div class="block is-flex" style="margin-bottom: 0px">
            <a class="navbar-item" href="https://jinseokbae.github.io">
              <span class="icon">
                <i class="fas fa-home"></i>
              </span>
            </a>
            <a class="navbar-item" href="https://github.com/jinseokbae">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
            </a>
          </div>
        </div>
      </div>

      <!-- license -->
      <div class="content has-text-centered" style="margin-top: 1.6rem">
        <p>
          This website is licensed under a
          <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
            >Creative Commons Attribution-ShareAlike 4.0 International License</a
          >
        </p>
      </div>
    </footer>
  </body>

</html>
